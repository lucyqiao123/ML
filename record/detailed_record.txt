文件：data_wash_label_index.py--->产出label_index.npy-->var_data_filter_re.py存储样本数据-->U_net.py
----------------------------------------------------------
一、数据清洗：复刻论文方法，筛选出10540个样本（前后10帧，一共20帧），原文是10672个，原因可能为我们样本为258次降水，他们是268次，而且有质检工序，可能样本会出现少许不同。
**方法：先用文章方法得到所有条前后10帧降水样本的对应筛选index(0/1,数据结构为[3w+,20,256,256] 筛选为1的有10540条
**筛选条件：本次降雨中必须至少包括20帧，不然一个样本也没有；且后10帧最大值area_thred大于256（每帧满足强度大于35dBZ的区域点数）
-->样本是否满足条件设为0/1，存为label_index,存为label_index，数据结构为[3w+,1]的npy文件 **此处为什么非要存下来而不是一个文件直接筛选，是为了减少内存运载，不然就必须加载每一个样本
-->依次读取满足筛选label_index为1的样本，进行标准化，总共样本[10540,20,256,256]
-->由于样本太大先储存，200条样本存一个文件，就能删除掉很多暂存变量，命名为dBZ_sample200,dBZ_sample400,...,dBZ_sample10540
---------------------------------------------------------
二、模型建立
1.先看文章的结构图和附件了解基本的架构和每层卷积的大小，然后github上找到一个复刻的代码U-net reference参考构建skip_connection
2.首先根据文章原文做残差块，然后拼encoder和decoder各七层，然后难点在于encoder和decoder之间的skip_connection和三变量输入
**最后解决方法是参考github上代码，将encoder单独定义为类，最后return7层的out，这样就能方便一个一个对应skip_connection
skip_connection和残差块构造不同，因为残差块是通道一致，数据结构也一致所以最后直接相加，但这个skip是三个变量输入和decoder一个变量大小数据做skip,所以skip_connection直接将两个out拼起来成为[4,channel*4,h,w]，再换通道卷积为[4,channel,h,w]，让模型自行选择更好的适应方式。
**三变量则分别包入forward里计算，在decoder中直接concat一起计算了
3.自定义数据集 MyDataset，发现关键问题在于idx和整体length,因为考虑数据量问题不能一次在__init__(self,vars,mode)这里加载，不然内存不够，
所以应该在__getitem__迭代加载数据，idx为DataLoader设置shuffle的时候随机取的样本index，这个index范围又是根据length设置的，所以要细细调整测试，看看样本是否正常能够加载出来。
--------此时已经能跑通，但是还没加注意力机制、Balanced-MSE、test指标CSI

4.加入注意力机制SE-Block确定三个变量合并起来的权重，自适应权重，下一步给decoder
----->为查看预测效果定义draw函数，画test最后一个样本的预测10帧和真实的对比图--->发现加注意力之后可视化效果确实更好，loss没有明显变化
--------此时加注意力机制，可视化效果更好

5.test指标CSI，CSI为真实值达到35dbZ之后，预测也为35dBZ的区域为hits，而分母为一个达到强度另一个没达到的区域之和+hits，不考虑强度都没达到的情况
这个为气象预测专用指标，由于test每个batch的平均CSI太小，因此直接取每个batch中每帧的最大值，train样本最高达到0.52，文章里最高拿出来的为0.6，基本实现。
--------目前问题还是test经过迭代的效果没明显提升，且test_loss小于train_loss，感觉有点奇怪，怀疑是test和train数据集分布不平衡，所以参考文章加B-MSE。

6.加入Balanced-MSE，调整数据不平衡问题，但是由于数据量太大，最后指标需要[batch,10*256*256],[10*256*256,10*256*256]的cov矩阵，太大了内存爆了，只能用一帧帧还要用下采样弄到[4,1,64,64]的做，但是效果一般，loss很大达到50+,由于算力限制，放弃B-MSE。
**解决方法：换了test数据集，发现test_loss小于train_loss了，应该是数据集选择的问题
-------------------------到此，发现数据集不均衡问题最大，train和test为1：1,是为了方便赶紧看到结果，最优epcoh为2,也就是第3个迭代。
-------------------------之前train全部跑的时候，有过拟合现象，因此还需要再跑再调，但基本结构完毕。之后过拟合可以调学习率啥的参数
